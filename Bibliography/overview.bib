% Encoding: UTF-8

@InProceedings{Saito2015,
  author    = {Saito, Masaki and Matsui, Yusuke},
  title     = {Illustration2{V}ec: A Semantic Vector Representation of Illustrations},
  booktitle = {SIGGRAPH Asia 2015 Technical Briefs},
  year      = {2015},
  series    = {SA '15},
  publisher = {ACM},
  location  = {Kobe, Japan},
  isbn      = {978-1-4503-3930-8},
  pages     = {5:1--5:4},
  doi       = {10.1145/2820903.2820907},
  url       = {http://doi.acm.org/10.1145/2820903.2820907},
  acmid     = {2820907},
  address   = {New York, NY, USA},
  articleno = {5},
  file      = {:Saito2015 - Illustration2Vec_ a Semantic Vector Representation of Illustrations.pdf:PDF},
  keywords  = {CNNs, illustration, search, visual similarity},
  numpages  = {4},
}

@InProceedings{Cao2017,
  author    = {Cao, Ying and Chan, Antoni B. and Lau, Rynson W. H.},
  title     = {Mining Probabilistic Color Palettes for Summarizing Color Use in Artwork Collections},
  booktitle = {SIGGRAPH Asia 2017 Symposium on Visualization},
  year      = {2017},
  series    = {SA '17},
  publisher = {ACM},
  location  = {Bangkok, Thailand},
  isbn      = {978-1-4503-5411-0},
  pages     = {1:1--1:8},
  doi       = {10.1145/3139295.3139296},
  url       = {http://doi.acm.org/10.1145/3139295.3139296},
  acmid     = {3139296},
  address   = {New York, NY, USA},
  articleno = {1},
  file      = {:Cao2017 - Mining Probabilistic Color Palettes for Summarizing Color Use in Artwork Collections.pdf:PDF},
  keywords  = {color palettes, probabilistic modeling, visual analytics},
  numpages  = {8},
}

@Article{Jin2017,
  author      = {Yanghua Jin and Jiakai Zhang and Minjun Li and Yingtao Tian and Huachun Zhu and Zhihao Fang},
  title       = {Towards the Automatic Anime Characters Creation with Generative Adversarial Networks},
  date        = {2017-08-18},
  eprint      = {1708.05509v1},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Automatic generation of facial images has been well studied after the Generative Adversarial Network (GAN) came out. There exists some attempts applying the GAN model to the problem of generating facial images of anime characters, but none of the existing work gives a promising result. In this work, we explore the training of GAN models specialized on an anime facial image dataset. We address the issue from both the data and the model aspect, by collecting a more clean, well-suited dataset and leverage proper, empirical application of DRAGAN. With quantitative analysis and case studies we demonstrate that our efforts lead to a stable and high-quality model. Moreover, to assist people with anime character design, we build a website (http://make.girls.moe) with our pre-trained model available online, which makes the model easily accessible to general public.},
  file        = {online:http\://arxiv.org/pdf/1708.05509v1:PDF;:Jin2017 - Towards the Automatic Anime Characters Creation with Generative Adversarial Networks.pdf:PDF},
  keywords    = {cs.CV},
}

@Article{Szegedy2016,
  author      = {Christian Szegedy and Sergey Ioffe and Vincent Vanhoucke and Alex Alemi},
  title       = {Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning},
  date        = {2016-02-23},
  eprint      = {1602.07261v2},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge},
  file        = {online:http\://arxiv.org/pdf/1602.07261v2:PDF;:Szegedy2016 - Inception V4, Inception ResNet and the Impact of Residual Connections on Learning.pdf:PDF},
  keywords    = {cs.CV},
}

@Article{Xie2016,
  author      = {Saining Xie and Ross Girshick and Piotr Doll√°r and Zhuowen Tu and Kaiming He},
  title       = {Aggregated Residual Transformations for Deep Neural Networks},
  date        = {2016-11-16},
  eprint      = {1611.05431v2},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.},
  file        = {online:http\://arxiv.org/pdf/1611.05431v2:PDF;:Xie2016 - Aggregated Residual Transformations for Deep Neural Networks.pdf:PDF},
  keywords    = {cs.CV},
}

@Article{He2015,
  author        = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  title         = {Deep Residual Learning for Image Recognition},
  date          = {2015-12-10},
  eprint        = {1512.03385v1},
  eprintclass   = {cs.CV},
  eprinttype    = {arXiv},
  __markedentry = {[leyht:]},
  abstract      = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  file          = {online:http\://arxiv.org/pdf/1512.03385v1:PDF;:He2015 - Deep Residual Learning for Image Recognition.pdf:PDF},
  keywords      = {cs.CV},
}

@Article{Szegedy2015,
  author        = {Christian Szegedy and Vincent Vanhoucke and Sergey Ioffe and Jonathon Shlens and Zbigniew Wojna},
  title         = {Rethinking the Inception Architecture for Computer Vision},
  date          = {2015-12-02},
  eprint        = {1512.00567v3},
  eprintclass   = {cs.CV},
  eprinttype    = {arXiv},
  __markedentry = {[leyht:6]},
  abstract      = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.},
  file          = {online:http\://arxiv.org/pdf/1512.00567v3:PDF;:Szegedy2015 - Rethinking the Inception Architecture for Computer Vision.pdf:PDF},
  keywords      = {cs.CV},
}

@thesis{Hermans:2276711,
  author        = "Hermans, Joeri and Spanakis, Gerasimos and M√∂ckel, Rico
                   and Baranowski, Zbigniew and Canali, Luca",
  title         = "{On Scalable Deep Learning and Parallelizing Gradient
                   Descent}",
  month         = "Jul",
  year          = "2017",
  reportNumber  = "CERN-THESIS-2017-103",
  url           = "http://cds.cern.ch/record/2276711",
  note          = "Presented 06 Jul 2017",
  abstract = {Speeding up gradient based methods has been a subject of interest over the past years with many practical applications, especially with respect to Deep Learning. Despite the fact that many optimizations have been done on a hardware level, the convergence rate of very large models remains problematic. Therefore, data parallel methods next to mini-batch parallelism have been suggested to further decrease the training time of parameterized models using gradient based methods. Nevertheless, asynchronous optimization was considered too unstable for practical purposes due to a lacking understanding of the underlying mechanisms. Recently, a theoretical contribution has been made which defines asynchronous optimization in terms of (implicit) momentum due to the presence of a queuing model of gradients based on past parameterizations. This thesis mainly builds upon this work to construct a better understanding why asynchronous optimization shows proportionally more divergent behavior when the number of parallel workers increases, and how this affects existing distributed optimization algorithms. Furthermore, using our redefinition of parameter staleness, we construct two novel techniques for asynchronous optimization, i.e., AGN and ADAG. This work shows that these methods outperform existing methods, and are more robust to (distributed) hyperparameterization contrary to existing distributed optimization algorithms such as DOWNPOUR, (A)EASGD, and DynSGD. Additionally, this thesis presents several smaller contributions. First, we show that the convergence rate of EASGD derived algorithms is impaired by an equilibrium condition. However, this equilibrium condition makes sure that EASGD does not overfit quickly. Finally, we introduce a new metric, temporal efficiency, to evaluate distributed optimization algorithms against each other.},
}

@Comment{jabref-meta: databaseType:biblatex;}
